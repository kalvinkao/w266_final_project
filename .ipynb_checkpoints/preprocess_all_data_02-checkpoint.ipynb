{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import collections\n",
    "import pandas as pd\n",
    "import csv\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Process Real Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a list of list of characters from the 5-star reviews\n",
    "def preprocess_review_series(review_series):\n",
    "    review_list = []\n",
    "    for new_review in review_series:\n",
    "        clipped_review = new_review[2:-1]\n",
    "        char_list = list(clipped_review.lower())\n",
    "        semifinal_review = []\n",
    "        last_char = ''\n",
    "        for ascii_char in char_list:\n",
    "            if ascii_char == '\\\\' or last_char == '\\\\':\n",
    "                pass\n",
    "            else:#an explicit check for ascii-characters not needed\n",
    "                #isascii = lambda s: len(s) == len(s.encode())\n",
    "                semifinal_review.append(ascii_char)\n",
    "            last_char = ascii_char\n",
    "        if len(semifinal_review) > 300:\n",
    "            final_review = ['<SOR>'] + semifinal_review + ['<EOR>']\n",
    "            #print(final_review)\n",
    "            review_list.append(final_review)\n",
    "    return review_list\n",
    "\n",
    "def get_review_series(review_path = '/home/kalvin_kao/yelp_challenge_dataset/review.csv'):\n",
    "    #review_path = '/home/kalvin_kao/yelp_challenge_dataset/review.csv'\n",
    "    review_df = pd.read_csv(review_path)\n",
    "    five_star_review_df = review_df[review_df['stars']==5]\n",
    "    #five_star_review_series = five_star_review_df['text']\n",
    "    return five_star_review_df['text']\n",
    "\n",
    "def get_business_list(business_path = '/home/kalvin_kao/yelp_challenge_dataset/business.csv'):\n",
    "    #business_path = '/home/kalvin_kao/yelp_challenge_dataset/business.csv'\n",
    "    return pd.read_csv(business_path)\n",
    "\n",
    "def split_train_test(review_list, training_samples, test_samples):\n",
    "    #pass in randomized review list\n",
    "    train_len = int(np.floor(0.8*len(review_list)))\n",
    "    test_len = int(np.floor(0.2*len(review_list)))\n",
    "    training_review_list = review_list[:train_len]\n",
    "    testing_review_list = review_list[-test_len:]\n",
    "    randomized_training_list = random.sample(training_review_list, training_samples)\n",
    "    randomized_testing_list = random.sample(testing_review_list, test_samples)\n",
    "    training_review_list = [item for sublist in randomized_training_list for item in sublist]\n",
    "    print(\"number of training characters\", len(training_review_list))\n",
    "    test_review_list = [item for sublist in randomized_testing_list for item in sublist]\n",
    "    print(\"number of test characters\", len(test_review_list))\n",
    "    return randomized_training_list, randomized_testing_list\n",
    "\n",
    "def make_train_test_data(five_star_review_series, training_samples=25000, test_samples=6250):\n",
    "    #fix randomization to prevent evaluation on trained samples\n",
    "    review_list = preprocess_review_series(five_star_review_series)\n",
    "    np.random.shuffle(review_list)\n",
    "    #split into 3 for 1) attack, 2) defense, and 3) GAN\n",
    "    one_third_len = int(np.floor(0.33*len(review_list)))\n",
    "    attack_review_list = review_list[:one_third_len]\n",
    "    #attack_training_samples = 25000\n",
    "    #attack_test_samples = 6250\n",
    "    defense_review_list = review_list[one_third_len:(2*one_third_len)]\n",
    "    #defense_training_samples = 25000\n",
    "    #defense_test_samples = 6250\n",
    "    gan_review_list = review_list[-one_third_len:]\n",
    "    #gan_training_samples = 25000\n",
    "    #gan_test_samples = 6250\n",
    "    #split and shuffle the data for attack\n",
    "    attack_training_review_list, attack_test_review_list = split_train_test(attack_review_list, training_samples, test_samples)\n",
    "    defense_training_review_list, defense_test_review_list = split_train_test(defense_review_list, training_samples, test_samples)\n",
    "    gan_training_review_list, gan_test_review_list = split_train_test(gan_review_list, training_samples, test_samples)\n",
    "    return attack_training_review_list, attack_test_review_list, defense_training_review_list, defense_test_review_list, gan_training_review_list, gan_test_review_list\n",
    "\n",
    "def make_vocabulary(dataset_list):\n",
    "    unique_characters = list(set().union(*dataset_list))\n",
    "    #unique_characters = list(set(training_review_list + test_review_list))\n",
    "    #vocabulary\n",
    "    char_dict = {w:i for i, w in enumerate(unique_characters)}\n",
    "    ids_to_words = {v: k for k, v in char_dict.items()}\n",
    "    return char_dict, ids_to_words\n",
    "\n",
    "def convert_to_ids(char_dict, review_list):\n",
    "    #convert to flat (1D) np.array(int) of ids\n",
    "    review_ids = [char_dict.get(token) for token in review_list]\n",
    "    return np.array(review_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_path = '/home/kalvin_kao/yelp_challenge_dataset/review.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training characters 17970067\n",
      "number of test characters 4549136\n",
      "number of training characters 18069407\n",
      "number of test characters 4558226\n",
      "number of training characters 17811993\n",
      "number of test characters 4519432\n"
     ]
    }
   ],
   "source": [
    "five_star_reviews = get_review_series(review_path)\n",
    "training_samples = 25000\n",
    "test_samples = 6250\n",
    "attack_training_review_list, attack_test_review_list, defense_training_review_list, defense_test_review_list, gan_training_review_list, gan_test_review_list = make_train_test_data(five_star_reviews, training_samples, test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training characters 2979\n",
      "number of test characters 733\n"
     ]
    }
   ],
   "source": [
    "#np.random.shuffle(generated_reviews)\n",
    "#generated_training_review_list, generated_test_review_list = split_train_test(generated_reviews, training_samples=20, test_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<SOR>a true hard fix!!! the owner, egg, olive and potato was classy, simple and ill take the time to even have found the food is damn again i've never been\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generated_training_review_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"artificial_train_data.csv\", \"w\") as output:\n",
    "    writer = csv.writer(output, lineterminator='\\n')\n",
    "    writer.writerows(generated_training_review_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"artificial_test_data.csv\", \"w\") as output:\n",
    "    writer = csv.writer(output, lineterminator='\\n')\n",
    "    writer.writerows(generated_test_review_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"split01_train_data_01.csv\", \"w\") as output:\n",
    "    writer = csv.writer(output, lineterminator='\\n')\n",
    "    writer.writerows(attack_training_review_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"split01_test_data_01.csv\", \"w\") as output:\n",
    "    writer = csv.writer(output, lineterminator='\\n')\n",
    "    writer.writerows(attack_test_review_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"split01_train_data_02.csv\", \"w\") as output:\n",
    "    writer = csv.writer(output, lineterminator='\\n')\n",
    "    writer.writerows(defense_training_review_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"split01_test_data_02.csv\", \"w\") as output:\n",
    "    writer = csv.writer(output, lineterminator='\\n')\n",
    "    writer.writerows(defense_test_review_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"split01_train_data_03.csv\", \"w\") as output:\n",
    "    writer = csv.writer(output, lineterminator='\\n')\n",
    "    writer.writerows(gan_training_review_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"split01_test_data_03.csv\", \"w\") as output:\n",
    "    writer = csv.writer(output, lineterminator='\\n')\n",
    "    writer.writerows(gan_test_review_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\"<EOR>\" in generated_reviews[95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generated_reviews[95].find('<EOR>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and Save Generated Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"baseline_logs_01.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename, 'r') as f:\n",
    "    all_logs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_reviews = []\n",
    "for log in all_logs:\n",
    "    if 'jsonPayload' in log.keys():\n",
    "        payload = log['jsonPayload']\n",
    "        if 'message' in payload.keys():\n",
    "            message = payload['message']\n",
    "            if \"SOR\" in message:\n",
    "                generated_reviews.append(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_review_list = []\n",
    "for review in generated_reviews:\n",
    "    clipped_review = review[5:]\n",
    "    if \"<EOR>\" in clipped_review:\n",
    "        eor_index = clipped_review.find('<EOR>')\n",
    "        clipped_review = clipped_review[:eor_index]\n",
    "    clipped_review_list.append(clipped_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_review_list = []\n",
    "for review in clipped_review_list:\n",
    "    #print(type(review))\n",
    "    char_list = list(review.lower())\n",
    "    semifinal_review = []\n",
    "    last_char = ''\n",
    "    for ascii_char in char_list:\n",
    "        if ascii_char == '\\\\' or last_char == '\\\\':\n",
    "            pass\n",
    "        else:\n",
    "            #isascii = lambda s: len(s) == len(s.encode())\n",
    "            semifinal_review.append(ascii_char)\n",
    "        last_char = ascii_char\n",
    "    #if len(semifinal_review) > 300:\n",
    "    final_review = ['<SOR>'] + semifinal_review + ['<EOR>']\n",
    "        #print(final_review)\n",
    "    generated_review_list.append(final_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training characters 2708\n",
      "number of test characters 634\n"
     ]
    }
   ],
   "source": [
    "np.random.shuffle(generated_review_list)\n",
    "generated_training_review_list, generated_test_review_list = split_train_test(generated_review_list, training_samples=20, test_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"gen01_train_data_01.csv\", \"w\") as output:\n",
    "    writer = csv.writer(output, lineterminator='\\n')\n",
    "    writer.writerows(generated_training_review_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"gen01_test_data_01.csv\", \"w\") as output:\n",
    "    writer = csv.writer(output, lineterminator='\\n')\n",
    "    writer.writerows(generated_test_review_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Processed and Split Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"attack_train_data.csv\", 'r') as csvfile:\n",
    "    counter = 0\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    #for row in reader:\n",
    "        #print(row)\n",
    "        #print()\n",
    "        #if counter > 10:\n",
    "            #break\n",
    "        #counter += 1\n",
    "    new_training_review_list = [item for sublist in reader for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e',\n",
       " 'd',\n",
       " ']',\n",
       " ' ',\n",
       " 'y',\n",
       " 'e',\n",
       " 'a',\n",
       " 'r',\n",
       " '-',\n",
       " 'o',\n",
       " 'l',\n",
       " 'd',\n",
       " ' ',\n",
       " 'm',\n",
       " 'e',\n",
       " ' ',\n",
       " 'a',\n",
       " 'n',\n",
       " 'd',\n",
       " ' ',\n",
       " 'p',\n",
       " 'u',\n",
       " 'n',\n",
       " 'c',\n",
       " 'h',\n",
       " ' ',\n",
       " 'm',\n",
       " 'e',\n",
       " ' ',\n",
       " 's',\n",
       " 'q',\n",
       " 'u',\n",
       " 'a',\n",
       " 'r',\n",
       " 'e',\n",
       " 'l',\n",
       " 'y',\n",
       " ' ',\n",
       " 'i',\n",
       " 'n',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'n',\n",
       " 'a',\n",
       " 'd',\n",
       " 's',\n",
       " ' ']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_training_review_list[100:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"artificial_train_data.csv\", 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    #for row in reader:\n",
    "        #print(row)\n",
    "        #print(''.join(row))\n",
    "        #print()\n",
    "    new_artificial_review_list = [item for sublist in reader for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t',\n",
       " 'h',\n",
       " 'e',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 'o',\n",
       " 'n',\n",
       " ' ',\n",
       " 'i',\n",
       " 't',\n",
       " 's',\n",
       " ' ',\n",
       " 'a',\n",
       " 'c',\n",
       " 't',\n",
       " 'y',\n",
       " ',',\n",
       " ' ',\n",
       " 'm',\n",
       " 'a',\n",
       " 'k',\n",
       " 'e',\n",
       " 's',\n",
       " ' ',\n",
       " 'y',\n",
       " 'o',\n",
       " 'u',\n",
       " ' ',\n",
       " 'f',\n",
       " 'e',\n",
       " 'e',\n",
       " 'l',\n",
       " ' ',\n",
       " 'v',\n",
       " 'e',\n",
       " 'r',\n",
       " 'y',\n",
       " ' ',\n",
       " 'a',\n",
       " 't',\n",
       " 't',\n",
       " 'e',\n",
       " 'n',\n",
       " 't',\n",
       " 'i',\n",
       " 'v',\n",
       " 'e',\n",
       " ',',\n",
       " ' ']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_artificial_review_list[100:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Character \"Vocabulary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_ids, ids_to_words = make_vocabulary([new_training_review_list, new_artificial_review_list])\n",
    "attack_train_ids = convert_to_ids(words_to_ids, new_training_review_list)\n",
    "artificial_train_ids = convert_to_ids(words_to_ids, new_artificial_review_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = dict(V=len(words_to_ids.keys()), H=1024, softmax_ns=len(words_to_ids.keys()), num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'H': 1024, 'V': 68, 'num_layers': 2, 'softmax_ns': 68}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trained_filename = run_training(train_ids, test_ids, tf_savedir = \"/tmp/artificial_hotel_reviews/a4_model\", model_params=model_params, max_time=150, batch_size=256, learning_rate=0.002, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trained_filename = run_training(train_ids, test_ids, tf_savedir = \"/tmp/artificial_hotel_reviews/a4_model\", model_params=model_params, max_time=150, batch_size=256, learning_rate=0.002, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_review_char_list = [item for sublist in clipped_review_list for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_chars = pd.Series(generated_review_char_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     13070\n",
       "unique       56\n",
       "top            \n",
       "freq       2376\n",
       "dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_chars.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#import collections\n",
    "print([item for item, count in collections.Counter(generated_reviews).items() if count > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_counts = collections.Counter(generated_review_char_list).items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('o', 762)\n",
      "('f', 286)\n",
      "(' ', 2376)\n",
      "('y', 253)\n",
      "('u', 288)\n",
      "('r', 602)\n",
      "('b', 156)\n",
      "('w', 252)\n",
      "('s', 646)\n",
      "('e', 1294)\n",
      "(',', 86)\n",
      "('a', 913)\n",
      "('t', 934)\n",
      "('l', 430)\n",
      "('h', 514)\n",
      "('i', 686)\n",
      "('n', 624)\n",
      "('k', 97)\n",
      "('g', 229)\n",
      "('c', 283)\n",
      "(\"'\", 34)\n",
      "('d', 400)\n",
      "('.', 147)\n",
      "('p', 210)\n",
      "('v', 112)\n",
      "('m', 235)\n",
      "('!', 63)\n",
      "('&', 2)\n",
      "('z', 21)\n",
      "('j', 20)\n",
      "('q', 13)\n",
      "('-', 15)\n",
      "('(', 14)\n",
      "(')', 7)\n",
      "('\"', 2)\n",
      "('=', 1)\n",
      "('x', 13)\n",
      "(';', 1)\n",
      "(':', 3)\n",
      "('1', 8)\n",
      "('8', 3)\n",
      "('4', 1)\n",
      "('5', 4)\n",
      "('2', 3)\n",
      "('*', 2)\n",
      "('7', 2)\n",
      "('+', 1)\n",
      "('?', 3)\n",
      "('{', 1)\n",
      "('$', 3)\n",
      "('#', 1)\n",
      "('0', 6)\n",
      "('6', 1)\n",
      "('3', 4)\n",
      "('9', 2)\n",
      "('/', 1)\n"
     ]
    }
   ],
   "source": [
    "for item in char_counts:\n",
    "    print(item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
